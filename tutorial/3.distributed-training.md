In this chapter, we are going to train vgg11 model with torchvision's cifar10 dataset using pytorch distributed training module and then replace it with slurm srun batch.

### 1. clone trainng code ###

after checking two nvidia worker nodes.
```
$ terraform output
client = "43.203.254.240"
graviton-workers = [
  "43.202.0.73",
  "3.36.97.84",
]
master = "13.125.65.133"
monitoring = [
  "prometheus url: ec2-54-180-86-63.ap-northeast-2.compute.amazonaws.com:8080",
  "grafana url: ec2-54-180-86-63.ap-northeast-2.compute.amazonaws.com:8081",
]
nvidia-workers = [
  "43.203.125.200",
  "3.34.140.81",
]
```
log into two nvidia worker nodes respectively, activate pytorch and clone workshop repository. 
```
ubuntu@slc-wn1$ source activate pytorch
ubuntu@slc-wn1$ git clone https://github.com/gnosia93/slurm-on-grv.git
ubuntu@slc-wn1$ cd slurm-on-grv/ddp
```
```
ubuntu@slc-wn2$ source activate pytorch
ubuntu@slc-wn2$ git clone https://github.com/gnosia93/slurm-on-grv.git
ubuntu@slc-wn2$ cd slurm-on-grv/ddp
```


### 2. train with torchrun ###
replace --master_addr with master node's private ip and execute torchrun at all worker nodes.   
all worker nodes couble be master node, but here we select slc-wn1 as a master. In this example, MASTER_ADDR is slc-wn1's private ip address. 
keep an eye on --node_rank value (master node is 0, others is increased by 1 each)
```
ubuntu@slc-wn1$ MASTER_ADDR=10.0.101.161 torchrun \
--nproc_per_node=1 --nnodes=2 --node_rank=0 \
--master_addr=$MASTER_ADDR --master_port=29400 \
cifar10-vgg.py

ubuntu@slc-wn2$ MASTER_ADDR=10.0.101.161 torchrun \
--nproc_per_node=1 --nnodes=2 --node_rank=1 \
--master_addr=$MASTER_ADDR --master_port=29400 \
cifar10-vgg.py
```
![](https://github.com/gnosia93/slurm-on-grv/blob/main/slurm/images/torchrun-1.png)



### 3. train with slurm ###

Comparing to torchrun which we have to execute torchrun script in all worker node, slurm use just client node for initiating distributed training. 
when we are executing slurm command at client node, required tasks for distributed training are all set by slurm scheduler.   
let's go into slurm-client node, and make train.sh file using below snippet.

[train.sh]
```
#!/bin/bash
#SBATCH --job-name=cifar10-vgg
#SBATCH --partition=nvidia
#SBATCH --time=1:00:00

### e.g. request 2 nodes with 1 gpu each, totally 2 gpus (WORLD_SIZE==2)
### Note: --gres=gpu:x should equal to ntasks-per-node
#SBATCH --nodes=2
#SBATCH --ntasks=2
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=4
#SBATCH --chdir=/home/ubuntu
#SBATCH --output=/home/ubuntu/%x-%j.out

export MASTER_PORT=29400
export WORLD_SIZE=2

### get the first node name as master address
### SLURM_NODELIST and SLURM_JOB_NODELIST are output variable of slurm
### Those values are set automatically when slurm job is running.
echo "NODELIST="${SLURM_NODELIST}
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
echo "MASTER_ADDR="$MASTER_ADDR

### init virtual environment if needed
### slurm master fetch workshop git repo in shared filesystem diretory(/mnt/efs),
### other worker nodes just read it. 
source activate pytorch
cd /mnt/efs
DIRECTORY=slurm-on-grv
WORKSHOP_REPO=https://github.com/gnosia93/slurm-on-grv.git
if [ -d "$DIRECTORY" ]; then
  echo "$DIRECTORY does exist."
  cd $DIRECTORY && git pull && cd ddp
else
  git clone $WORKSHOP_REPO
  cd slurm-on-grv/ddp
fi

### the command to run
srun python cifar10-vgg.py
```
see there is no --node_rank parameter in slurm batch script, for slurm to refer rank information, you have to modify training source code (cifar10-vgg.py).
check cifar10-vgg.py source code of setup() and train() code block 
```
def setup():
    if 'SLURM_PROCID' in os.environ:     # for slurm scheduler
        global_rank = int(os.environ['SLURM_PROCID'])
        dist.init_process_group("nccl", rank=global_rank)
    else:                                # for torchrun
        dist.init_process_group("nccl")
```
```
def train():
    global_rank = dist.get_rank()        # for torchrun    
    if 'SLURM_PROCID' in os.environ:     # for slurm scheduler
        global_rank = int(os.environ['SLURM_PROCID'])
```

and execute train.sh with sbatch and observe slurm job with scontrol and squeue.
```
ubuntu@slc-client:~$ sbatch train.sh
ubuntu@slc-client:~$ scontrol show job
ubuntu@slc-client:~$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
                26    nvidia cifar10-   ubuntu  R       4:50      2 slc-wn[1-2]
```


## reference ##

* https://stackoverflow.com/questions/71679376/mpi-bind-ranks-to-specific-nodes-via-slurm
* [Multi-node-training on slurm with PyTorch](https://gist.github.com/TengdaHan/1dd10d335c7ca6f13810fff41e809904)
* https://sites.google.com/nyu.edu/nyu-hpc/training-support/tutorials/slurm-tutorial#h.yhmo5ppr1s3x

