In this chapter, we are going to train tiny model with pytorch distributed training module.
and then we will replace it with slurm batch.

### 1. clone trainng code ###

after checking two nvidia worker nodes.
```
$ terraform output
nvidia-workers = [
  "54.180.132.197",
  "3.34.51.81",
]
```
log into two nvidia worker nodes respectively, activate pytorch and clone workshop repository. 
```
ubuntu@slc-wn1$ source activate pytorch
ubuntu@slc-wn1$ git clone https://github.com/gnosia93/slurm-on-grv.git
ubuntu@slc-wn1$ cd slurm-on-grv/ddp
```


### 2. train with torchrun ###
replace --master_addr with master node's private ip and execute torchrun at all worker nodes.   
all worker nodes couble be master node, but here we select slc-wn1 as a master. In this example, MASTER_ADDR is slc-wn1's private ip address. 
keep an eye on --node_rank value (master node is 0, others is increased by 1 each)
```
ubuntu@slc-wn1$ MASTER_ADDR=10.0.101.161 torchrun \
--nproc_per_node=1 --nnodes=2 --node_rank=0 \
--master_addr=$MASTER_ADDR --master_port=29400 \
cifar10-vgg.py

ubuntu@slc-wn2$ MASTER_ADDR=10.0.101.161 torchrun \
--nproc_per_node=1 --nnodes=2 --node_rank=1 \
--master_addr=$MASTER_ADDR --master_port=29400 \
cifar10-vgg.py
```
![](https://github.com/gnosia93/slurm-on-grv/blob/main/slurm/images/torchrun-1.png)



### 3. train with slurm ###

Comparing to torchrun which we have to execute torchrun script in all worker node, slurm use just client node for initiating distributed training. 
when we are executing slurm command at client node, required tasks for distributed training are all set by slurm scheduler.   
let's log into slurm-client node, and train.sh with below script

```
#!/bin/bash
#SBATCH --job-name=cifar10-vgg
#SBATCH --partition=nvidia
#SBATCH --time=1:00:00

### e.g. request 2 nodes with 1 gpu each, totally 2 gpus (WORLD_SIZE==2)
### Note: --gres=gpu:x should equal to ntasks-per-node
#SBATCH --nodes=2
#SBATCH --ntasks=2
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=4
#SBATCH --chdir=/home/ubuntu
#SBATCH --output=/home/ubuntu/%x-%j.out

export MASTER_PORT=29400
export WORLD_SIZE=2

### get the first node name as master address
### SLURM_NODELIST and SLURM_JOB_NODELIST are output variable of slurm
### Those values are set automatically when slurm job is running.
echo "NODELIST="${SLURM_NODELIST}
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
echo "MASTER_ADDR="$MASTER_ADDR

### init virtual environment if needed
### slurm master fetch workshop git repo in shared filesystem diretory(/mnt/efs),
### other worker nodes just read it. 
source activate pytorch
cd /mnt/efs
DIRECTORY=slurm-on-grv
WORKSHOP_REPO=https://github.com/gnosia93/slurm-on-grv.git
if [ -d "$DIRECTORY" ]; then
  echo "$DIRECTORY does exist."
  cd $DIRECTORY && git pull && cd ddp
else
  git clone $WORKSHOP_REPO
  cd slurm-on-grv/ddp
fi

### the command to run
srun python cifar10-vgg.py
```
see there is no --node_rank parameter, in order to slurm refer rank information, you have to modify training source code (cifar10-vgg.py) 


and execute train.sh with sbatch and observe slurm job with scontrol and squeue.
```
ubuntu@slc-client:~$ sbatch train.sh
ubuntu@slc-client:~$ scontrol show job
ubuntu@slc-client:~$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
                26    nvidia cifar10-   ubuntu  R       4:50      2 slc-wn[1-2]
```


## reference ##

* https://stackoverflow.com/questions/71679376/mpi-bind-ranks-to-specific-nodes-via-slurm
* [Multi-node-training on slurm with PyTorch](https://gist.github.com/TengdaHan/1dd10d335c7ca6f13810fff41e809904)
* https://sites.google.com/nyu.edu/nyu-hpc/training-support/tutorials/slurm-tutorial#h.yhmo5ppr1s3x

